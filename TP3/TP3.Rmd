---
title: "R Notebook"
output: html_notebook
---
# TP3

## Pré-traitement

### Récupération des données

importation des modules
```{r}
library(Matrix)
library(NLP)
library(tm)
library(irlba)
library(stats)
library(pROC)
library(parallel)
```

On construit des objects SimpleCorpus du package Text Mining

Sur ces coprus, on supprime les nombres, la ponctuation, et on tranfsorme toutes les majuscules en minuscules
```{r}
get_data <- function(src) {
  corpus <- SimpleCorpus(DirSource(src))
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(tolower))
  return(corpus)
}

corpus_poly <- get_data('data/Poly')
corpus_udm <- get_data('data/UdM')
corpus_hec <- get_data('data/HEC')
corpus_uqam <- get_data('data/UQAM')

head(data.frame(text = sapply(corpus_poly, as.character)))
```
On va ensuite construire un grand corpus regroupant les corpus des 4 écoles

```{r}
corpus_tot <- get_data(c('data/Poly', 'data/UdM', 'data/HEC', 'data/UQAM'))
```

### matrices Termes-Documents

On utilise le package TM pour construire des matrices termes-documents

On supprime avant cela les "stop words", mots très fréquents et que l'on retrouve dans tout les corpus de textes francais. Ces mots ne comportent pas de sémantique particulière (comme les prépositions, etc...)
On supprime aussi les deux groupe nominaux "titrecours" et "descriptioncours" qui seront en quantité égale dans tout les documents et qui par conséquents ne sont pas utiles dans ce travail
```{r}
to_matrix <- function(corpus) {
  corpus <- tm_map(corpus,removeWords, stopwords('french'))
  corpus <- tm_map(corpus, removeWords, c('titrecours', 'descriptioncours'))
  return(TermDocumentMatrix(corpus))
}

td_poly <- to_matrix(corpus_poly)
td_udm <- to_matrix(corpus_udm)
td_hec <- to_matrix(corpus_hec)
td_uqam <- to_matrix(corpus_uqam)

td_tot <- to_matrix(corpus_tot)
```
```{r}
as.data.frame(as.matrix(td_tot[0:10, 0:10]))
```

Une fois la matrice terme-document définie, on va définir la matrice Tf-Idf

```{r}
get_Tf_IDF <- function(td) {
  tf <- as.matrix(td)
  tf_copy <- tf
  tf_copy[tf_copy != 0] <- 1
  n_i <- rowSums(tf_copy)
  idf <- log(nDocs(td_tot)/n_i)
  tf_idf = tf * idf
  return(tf_idf)
}

tf_idf <- Matrix(get_Tf_IDF(td_tot), "dgCMatrix")
```
```{r}
as.data.frame(as.matrix(tf_idf[, 0:9]))
```

On reduis le nombre de dimention de la matrice. On transposera la matrice auparavant pour obtenir une matrice document-terme
```{r}
dim_redu = 50

m.svd <- irlba(t(tf_idf), 50)
reduce_tf_idf <- m.svd$u[, 1:dim_redu] %*% diag(m.svd$d[1:dim_redu])
rownames(reduce_tf_idf) <- colnames(tf_idf)
```
```{r}
as.data.frame(as.matrix(reduce_tf_idf[, 1:9]))
```

## Question 1 

**Quels sont les 10 cours les plus similaires à LOG2420 dans l'espace réduit à 50 dimensions? **

On va dans un premier temps récupérer les fonctions de similarité de la correction du TP1
```{r}
## Cosinus entre un vecteur v et chaque colonne dela matrice m
cosinus.vm <- function(v,m) { n <- sqrt(colSums(m^2)); (v %*% m)/(n * sqrt(sum(v^2))) }
## Cosinus des colonnes d'une matrice
cosinus.mm <- function(m) { n <- sqrt(colSums(m^2)); crossprod(m)/(n %o% n) }
# Trouve les indexes des premières 'n' valeurs maximales d'une matrice
max.nindex <- function(m, n=5) {
  i <- order(m, decreasing=TRUE)
  return(i[1:n])
}
min.nindex <- function(m, n=5) {
  i <- order(m)
  return(i[1:n])
}
```

On va ensuite sélectionner l'indice du cours LOG2420
```{r}
i <- grep('LOG2420', as.character(rownames(reduce_tf_idf)))
```

On va enfin calculer la similarité des documents entre-eux, avec la méthode des cosinus (on transpose la matrice pour obtenir une matrice terme-document)
```{r}
reduce_tf_idf.cos <- cosinus.mm(t(reduce_tf_idf))
```
```{r}
head(as.data.frame(as.matrix(reduce_tf_idf.cos[,1:6])))
```
On peut noter que la diagonale est bien composée de 0

on sélectionne les 10 cours les plus proches de LOG2420
```{r}
i.sim.cos <- max.nindex(reduce_tf_idf.cos[,i], 11)
data.frame(Cos=reduce_tf_idf.cos[i.sim.cos, i], Index=i.sim.cos)
```

## Question 2

**Effectuez une classification de cours par une approche supervisée**

Dans un premier temps on va sélectionner les index des cours qui nous intéressent
```{r}
reduce_tf_idf.class <- subset(reduce_tf_idf, grepl("PSY|PHY", rownames(reduce_tf_idf)))

reduce_tf_idf.PSY <- subset(reduce_tf_idf, grepl("PSY", rownames(reduce_tf_idf)))

reduce_tf_idf.PHY <- subset(reduce_tf_idf, grepl("PHY", rownames(reduce_tf_idf)))
```

Voila le résultat pour la classe PSY
```{r}
as.data.frame(as.matrix(reduce_tf_idf.PSY[,0:9]))
```

On va maintenant calculer l'erreur des deux classes respectives:

- On sépare les données en entrainement et en test (le code est tiré de la correction du TP 1)
```{r}
nfolds = 3
split_data <- function (nfolds, fold, m) {
  split <- rep(1:nfolds, length.out=nrow(m))[sample(nrow(m))]
  m.test.i <- (split==fold)
  m.train.i <- !m.test.i
  
  m.train <- m[m.train.i,]
  m.test <- m[m.test.i,]
  return(list("train" = m.train, "train.i" = m.train.i, "test" = m.test, "test.i" = m.test.i))
}
splited_tf_idf.PSY <- split_data(nfolds, nfolds, reduce_tf_idf.PSY)
splited_tf_idf.PHY <- split_data(nfolds, nfolds, reduce_tf_idf.PHY)
```
```{r}
data.frame(data_set = c("train PSY", "train PHY", "test PSY", "test PHY"), "nb_cours,nb_vecteur " = c(toString(dim(splited_tf_idf.PSY$train)),
                                                                                    toString(dim(splited_tf_idf.PHY$train)),
                                                                                    toString(dim(splited_tf_idf.PSY$test)),
                                                                                    toString(dim(splited_tf_idf.PHY$test))))
```

On va ensuite calculer le centroides de la classe
```{r}
centroide.PSY <- colMeans(splited_tf_idf.PSY$train)
centroide.PHY <- colMeans(splited_tf_idf.PHY$train)
```
```{r}
data.frame(t(cbind(centroide.PHY, centroide.PSY)))
```

On va maintenant calculer l'erreur générée par cet itération

Pour chaque vecteur correspondant à une description de cours, on va calculer sa similarité cosinus avec les deux centroides. On prédira la classe en sélectionnant le centroide le plus proche:
- on calcule les similaritée pour chaque cours à chaque centroide
```{r}
prediction.PSY <- data.frame(cours = rownames(splited_tf_idf.PSY$test),
                                     sim.PSY = as.double(cosinus.vm(centroide.PSY, t(splited_tf_idf.PSY$test))),
                                     sim.PHY = as.double(cosinus.vm(centroide.PHY, t(splited_tf_idf.PSY$test))))
head(prediction)
```

- on  normalise les similarités pour obtenir des probabilitées
```{r}
prediction.PSY$pred.PSY <- apply(prediction.PSY, 1, function (v) {as.numeric(v['sim.PSY']) / (as.numeric(v['sim.PHY']) + as.numeric(v['sim.PSY']))})
prediction.PSY$pred.PHY <- apply(prediction.PSY, 1, function (v) {as.numeric(v['sim.PHY']) / (as.numeric(v['sim.PHY']) + as.numeric(v['sim.PSY']))})
head(prediction)
```

- on affiche les classes réelles pour simplifier le calcul de la courbe ROC
```{r}
prediction.PSY$is.PSY <- 1
prediction.PSY$is.PHY <- 0
head(prediction)
```

On fait la même chose pour PHY
```{r}
prediction.PHY <- data.frame(cours = rownames(splited_tf_idf.PHY$test),
                                     sim.PSY = as.double(cosinus.vm(centroide.PSY, t(splited_tf_idf.PHY$test))),
                                     sim.PHY = as.double(cosinus.vm(centroide.PHY, t(splited_tf_idf.PHY$test))))

prediction.PHY$pred.PHY <- apply(prediction.PHY, 1, function (v) {as.numeric(v['sim.PHY']) / (as.numeric(v['sim.PHY']) + as.numeric(v['sim.PSY']))})
prediction.PHY$pred.PSY <- apply(prediction.PHY, 1, function (v) {as.numeric(v['sim.PSY']) / (as.numeric(v['sim.PHY']) + as.numeric(v['sim.PSY']))})

prediction.PHY$is.PHY <- 1
prediction.PHY$is.PSY <- 0
```

Maintenant on peut construire des vecteurs pour fournir à la fonction ROC
```{r}
prediction.class <- rbind(prediction.PHY, prediction.PSY)

responce.PSY <- roc(prediction.class$is.PSY, prediction.class$pred.PSY, plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE, levels=c(0,1), direction="<")
responce.PHY <- roc(prediction.class$is.PHY, prediction.class$pred.PHY, plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE, levels=c(0,1), direction="<")
```
On peut remarquer que les deux courbes sont symétriques par rapport à x=-y + 1. C'est normal car les deux probabilitées sont liées: P(psy) = 1 - P(phy)

Nous allons maintenant répeter toutes ces étapes n fois, en fonction du nombre de fold (plie) sélectionné
```{r}
get_auc <- function (m.PSY, m.PHY, nfolds) {
  result <- sapply(1:nfolds , function(fold) {
  
    # construction des ensemble d'apprentissage et de test
    splited_m.PSY <- split_data(nfolds, fold, m.PSY)
    splited_m.PHY <- split_data(nfolds, fold, m.PHY)
    
    # on determine les centroides
    centroide.PSY <- colMeans(splited_m.PSY$train)
    centroide.PHY <- colMeans(splited_m.PHY$train)
    
    # on construit un nouveau data frame pour PSY
    prediction.PSY <- data.frame(cours = rownames(splited_m.PSY$test),
                                         sim.PSY = as.double(cosinus.vm(centroide.PSY, t(splited_m.PSY$test))),
                                         sim.PHY = as.double(cosinus.vm(centroide.PHY, t(splited_m.PSY$test))))
    
    prediction.PSY$pred.PSY <- apply(prediction.PSY, 1, function (v) {as.numeric(v['sim.PSY']) / (as.numeric(v['sim.PHY']) + as.numeric(v['sim.PSY']))})
    prediction.PSY$pred.PHY <- apply(prediction.PSY, 1, function (v) {as.numeric(v['sim.PHY']) / (as.numeric(v['sim.PHY']) + as.numeric(v['sim.PSY']))})
    prediction.PSY$is.PSY <- 1
    prediction.PSY$is.PHY <- 0
    
    # on construit un nouveau data frame pour PHY
    prediction.PHY <- data.frame(cours = rownames(splited_m.PHY$test),
                                         sim.PSY = as.double(cosinus.vm(centroide.PSY, t(splited_m.PHY$test))),
                                         sim.PHY = as.double(cosinus.vm(centroide.PHY, t(splited_m.PHY$test))))
    
    prediction.PHY$pred.PHY <- apply(prediction.PHY, 1, function (v) {as.numeric(v['sim.PHY']) / (as.numeric(v['sim.PHY']) + as.numeric(v['sim.PSY']))})
    prediction.PHY$pred.PSY <- apply(prediction.PHY, 1, function (v) {as.numeric(v['sim.PSY']) / (as.numeric(v['sim.PHY']) + as.numeric(v['sim.PSY']))})
    prediction.PHY$is.PHY <- 1
    prediction.PHY$is.PSY <- 0
    
    # on recupère la valeur de AUC
    prediction.class <- rbind(prediction.PHY, prediction.PSY)
    responce.PSY <- roc(prediction.class$is.PSY, prediction.class$pred.PSY, levels=c(0,1), direction="<")
  })
  return(result)
}

results <- as.data.frame(t(get_auc(reduce_tf_idf.PSY, reduce_tf_idf.PHY, 3)))
mean(as.double(results$auc))
```
A partir de la fonction supérieure, nous allons essayer plusieures dimentions de SVD.

```{r}
get_auc_by_dimention <- function(dim_redu, tf_idf) {
  
  m.svd <- irlba(t(tf_idf), dim_redu)
  reduce_tf_idf <- m.svd$u[, 1:dim_redu] %*% diag(m.svd$d[1:dim_redu])
  rownames(reduce_tf_idf) <- colnames(tf_idf)

  reduce_tf_idf.PSY <- subset(reduce_tf_idf, grepl("PSY", rownames(reduce_tf_idf)))
  reduce_tf_idf.PHY <- subset(reduce_tf_idf, grepl("PHY", rownames(reduce_tf_idf)))
  
  results <- as.data.frame(t(get_auc(reduce_tf_idf.PSY, reduce_tf_idf.PHY, 3)))
  return(mean(as.double(results$auc)))
}
dim_min = 2
dim_max = 50
auc_by_dimentions <- mclapply(dim_min:dim_max, get_auc_by_dimention, tf_idf, mc.cores = detectCores())
plot(dim_min:dim_max, auc_by_dimentions, ylab = "AUC", xlab = "nombre de dimensions")
```
On peut voir qu'a partir d'une dimentions de 30, les valeurs d'AUC sont presque parfaite. On peut donc se contenter d'une dimention de 30.

## Question 3

**Effectuez une agglomération par k-means (k=2) et vérifiez si les classes PHY et PSY sont bien séparées par cette méthode**

On s'assure qu'il n'y ai pas de valeur manquante
```{r}
sum(reduce_tf_idf.class[is.na(reduce_tf_idf.PSY)])
```

On peut donc proccéder au K-mean sans se soucier des valeures manquantes
On va d'abord créer un ensemble de test et d'entrainement
```{r}
# apply_k_mean <- function(reduce_tf_idf.class) {
nfolds = 3
splited_tf_idf.class <- split_data(nfolds, nfolds, reduce_tf_idf.class)
```

On exécute l'algorithme k-mean sur l'ensemble de test
```{r}
k_mean_res <- kmeans(splited_tf_idf.class$train, 2)

cluster.class <- data.frame(names = rownames(splited_tf_idf.class$train), cluster = as.numeric(k_mean_res$cluster))
head(cluster.class)
```

On esssaye maintenant de deviner quel cluster correspond le mieux aux classes PHY et PSY:
- on recupère les indices des classes réelles
```{r}
index_PHY = grepl("PHY", cluster.class$names)
index_PSY = grepl("PSY", cluster.class$names)

cluster.class$is.PHY <- index_PHY
cluster.class$is.PSY <- index_PSY
```

- on fait la moyenne des numéros de cluster pour chacune des classes
```{r}
mean_cluster_PHY <- mean(cluster.class$cluster[index_PHY])
mean_cluster_PSY <- mean(cluster.class$cluster[index_PSY])
data.frame(classe_reelle = c("PHY", "PSY"), moyenne_des_clusters = c(mean_cluster_PHY, mean_cluster_PSY))
```

On attribue le numéro du cluster à sa classe la plus proche
```{r}
num_cluster_PHY <- if(mean_cluster_PHY < mean_cluster_PSY) 1 else 2
num_cluster_PSY <- if(mean_cluster_PSY < mean_cluster_PHY) 1 else 2
data.frame(classe_reelle = c("PHY", "PSY"), moyenne_des_clusters = c(num_cluster_PHY, num_cluster_PSY))
```

On peut maitenant attribuer un centroide aux classes
```{r}
centroide.PHY <- k_mean_res$centers[num_cluster_PHY,]
centroide.PSY <- k_mean_res$centers[num_cluster_PSY,]
data.frame(t(cbind(centroide.PHY, centroide.PSY)))
```

On peut prévoir comme auparavant les classes par la similarité de chaque élement de l'ensemble de test et des deux centroides
- on calcule la similarité de chaque elements à chaque centroide
```{r}
prediction <- data.frame(cours = rownames(splited_tf_idf.class$test),
                                     sim.PSY = as.double(cosinus.vm(centroide.PSY, t(splited_tf_idf.class$test))),
                                     sim.PHY = as.double(cosinus.vm(centroide.PHY, t(splited_tf_idf.class$test))))
head(prediction)
```

- on normalise entre 0 et 1 pour obtenir une probabilité
```{r}
prediction$pred.PSY <- apply(prediction, 1, function (v) {as.numeric(v['sim.PSY']) / (as.numeric(v['sim.PHY']) + as.numeric(v['sim.PSY']))})
prediction$pred.PHY <- apply(prediction, 1, function (v) {as.numeric(v['sim.PHY']) / (as.numeric(v['sim.PHY']) + as.numeric(v['sim.PSY']))})
head(prediction)
```

On ajoute une colonne de label des classes réelles pour faciliter la construction de la courbe de ROC par la suite
```{r}
prediction$is.PSY <- 0
prediction$is.PHY <- 0
prediction$is.PSY[grepl("PSY", prediction$cours)] <- 1
prediction$is.PHY[grepl("PHY", prediction$cours)] <- 1
head(prediction)
```

On peut analyser un peu ces résultats
- on ajoute une colonne pour savoir si la prédiction est bonne
```{r}
prediction$pred.right <- apply(prediction, 1, function(v) {(v['pred.PSY'] <= v['pred.PHY'] && v['is.PHY'] == 1) || (v['pred.PHY'] <= v['pred.PSY'] && v['is.PSY'] == 1)})
head(prediction)
```

On calcul toutes les données nécessaire à l'obtention de la précision, du rappel et du F1 score
```{r}
view_prediction <- function(prediction_m) {
  prediction_m$pred.right <- apply(prediction_m, 1, function(v) {(v['pred.PSY'] <= v['pred.PHY'] && v['is.PHY'] == 1) || (v['pred.PHY'] <= v['pred.PSY'] && v['is.PSY'] == 1)})
  
  TP.PSY <- length(which(prediction_m$pred.right==TRUE & prediction_m$is.PSY == 1))
  FP.PSY <- length(which(prediction_m$pred.right==FALSE & prediction_m$is.PSY == 0))
  TN.PSY <- length(which(prediction_m$pred.right==TRUE & prediction_m$is.PSY == 0))
  FN.PSY <- length(which(prediction_m$pred.right==FALSE & prediction_m$is.PSY == 1))
    
  TP.PHY <- length(which(prediction_m$pred.right==TRUE & prediction_m$is.PHY == 1))
  FP.PHY <- length(which(prediction_m$pred.right==FALSE & prediction_m$is.PHY == 0))
  TN.PHY <- length(which(prediction_m$pred.right==TRUE & prediction_m$is.PHY == 0))
  FN.PHY <- length(which(prediction_m$pred.right==FALSE & prediction_m$is.PHY == 1))
    
  acc.PHY <- sum(prediction_m$pred.right[prediction_m$is.PHY]) / length(prediction_m$is.PHY)
  acc.PSY <- sum(prediction_m$pred.right[prediction_m$is.PSY]) / length(prediction_m$is.PSY)
  acc <- sum(prediction_m$pred.right) / nrow(prediction_m)
  
  precision.PSY <- TP.PSY / (TP.PSY + FP.PSY)
  precision.PHY <- TP.PHY / (TP.PHY + FP.PHY)
  precision <- (TP.PHY + TP.PSY) / (TP.PHY + FP.PHY + TP.PSY + FP.PSY)
  
  recall.PSY <- TP.PSY / (TP.PSY + FN.PSY)
  recall.PHY <- TP.PHY / (TP.PHY + FN.PHY)
  recall <- (TP.PSY + TP.PHY) / (TP.PSY + FN.PSY + TP.PHY + FN.PHY)
  
  F1_score.PSY <- 2 * precision.PSY * recall.PSY / (precision.PSY + recall.PSY)
  F1_score.PHY <- 2 * precision.PHY * recall.PHY / (precision.PHY + recall.PHY)
  F1_score <- 2 * precision * recall / (precision + recall)
  
  data.frame(classe = c("PHY", "PSY", "total"), accuracy = c(acc.PHY, acc.PSY, acc), precision = c(precision.PHY, precision.PSY, precision), recall = c(recall.PHY, recall.PSY, recall), F1_score = c(F1_score.PHY, F1_score.PSY, F1_score)) 
}
```
```{r}
view_prediction(prediction)
```
On peut observer plusieurs chose:
- les deux classes sont deséquilibrées en accuracy, en précision et en recall
- la précision est de presque 1 pour la classe PSY, mais le recall est médiocre
- le recall est de presque 1 pour la classe PHY, mais la précision est médiocre

On peut maintenant construire une courbe ROC et une valeur d'AUC
```{r}
prediction.roc <- roc(prediction$is.PSY, prediction$pred.PSY, levels=c(0,1), direction="<", plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE, print.auc=TRUE, show.thres=TRUE)
```
On obtient une valeur équivalente à la classification supervisée

On va maintenant mettre tout cela dans une fonction et faire des essais pour plusieures valeures de nombre de dimentions
```{r}
apply_k_mean <- function(m, nfolds, fold) {
  # sépration des données d'entrainement et de test
  splited_m <- split_data(nfolds, fold, m)
  
  # algorithme k-mean
  k_mean_res <- kmeans(splited_m$train, 2)
  
  # on attribut un numéro de cluster à chaque classe
  cluster.class <- data.frame(names = rownames(splited_m$train), cluster = as.numeric(k_mean_res$cluster))
  
  index_PHY = grepl("PHY", cluster.class$names)
  index_PSY = grepl("PSY", cluster.class$names)
  
  cluster.class$is.PHY <- index_PHY
  cluster.class$is.PSY <- index_PSY
  
  mean_cluster_PHY <- mean(cluster.class$cluster[index_PHY])
  mean_cluster_PSY <- mean(cluster.class$cluster[index_PSY])
  
  num_cluster_PHY <- if(mean_cluster_PHY < mean_cluster_PSY) 1 else 2
  num_cluster_PSY <- if(mean_cluster_PSY < mean_cluster_PHY) 1 else 2
  
  # on calcul les centroides
  centroide.PHY <- k_mean_res$centers[num_cluster_PHY,]
  centroide.PSY <- k_mean_res$centers[num_cluster_PSY,]
  
  # on ajoute les similaritées
  prediction <- data.frame(cours = rownames(splited_m$test),
                                     sim.PSY = as.double(cosinus.vm(centroide.PSY, t(splited_m$test))),
                                     sim.PHY = as.double(cosinus.vm(centroide.PHY, t(splited_m$test))))
  # on ajoute les prédictions
  prediction$pred.PSY <- apply(prediction, 1, function (v) {as.numeric(v['sim.PSY']) / (as.numeric(v['sim.PHY']) + as.numeric(v['sim.PSY']))})
  prediction$pred.PHY <- apply(prediction, 1, function (v) {as.numeric(v['sim.PHY']) / (as.numeric(v['sim.PHY']) + as.numeric(v['sim.PSY']))})

  # on ajoute les classes réelles
  prediction$is.PSY <- 0
  prediction$is.PHY <- 0
  prediction$is.PSY[grepl("PSY", prediction$cours)] <- 1
  prediction$is.PHY[grepl("PHY", prediction$cours)] <- 1

  return(prediction)
}
```

On peut essayer d'améliorer les valeurs d'accuracy et de précision obtenues précédement
Une bonne manière d'effectuer cela est d'équilibrer le nombre d'element par classe dans l'ensemble d'entrainement
```{r}
nb_max_cours <- min(nrow(reduce_tf_idf.PSY), nrow(reduce_tf_idf.PHY))
nb_max_cours
```
Nous allons donc limiter nos classes à 114 élements par classe

On construit une matrice réduite contenant le même nombre d'élements des deux classes
```{r}
reduce_tf_idf.class.equi <- rbind(reduce_tf_idf.PSY[1:nb_max_cours,], reduce_tf_idf.PHY[1:nb_max_cours,])
```

On applique la fonction définie précédement pour obtenir les prédictions par k-mean
```{r}
prediction.equi <- apply_k_mean(reduce_tf_idf.class.equi, 5, 5)
view_prediction(prediction.equi)
```
Cela a grandement amélioré  nos différents indices

On crée une fonction pour effecteur la k-fold cross validation
```{r}
cross_validation_k_mean <- function(m, nfolds) {
  # on applique k fois l'algorithme sur les k sous ensemble test-entrainement
  result <- lapply(1:nfolds , function(fold) {
    # une application de l'algorithme sur un ensemble test-validation
    prediction <- apply_k_mean(m, nfolds, fold)
    prediction.roc <- roc(prediction$is.PSY, prediction$pred.PSY, levels=c(0,1), direction="<")
    return(prediction.roc$auc)
  })
}

results <- cross_validation_k_mean(reduce_tf_idf.class, 5)
results.equi <- cross_validation_k_mean(reduce_tf_idf.class.equi, 5)
data.frame(essais = 1:5, AUC = as.double(results), AUC_equi = as.double(results.equi))
```
On peut voir que l'algorithme K-mean n'arrive pas  toujours départager les deux clusters (quelques résultats à 0.70), ce qui n'est plus le cas pour lee données équilibrées en nombre de classe

## Question 4

**À partir de la validation croisée de la tâche de classification ci-dessus, déterminez le nombre de dimensions latentes optimal de SVD selon une approche dite wrapper**

On réalise la même étude sur les dimentions que précedement à partir des fonctions définies ci-dessus
```{r}
get_auc_by_dimention_kmean <- function(dim_redu, tf_idf) {
  
  # réduction de dimention de la matrice TF-iDF
  m.svd <- irlba(t(tf_idf), dim_redu)
  reduce_tf_idf <- m.svd$u[, 1:dim_redu] %*% diag(m.svd$d[1:dim_redu])
  rownames(reduce_tf_idf) <- colnames(tf_idf)
  
  # on selectionne les classes PSY et PHY
  reduce_tf_idf.class <- subset(reduce_tf_idf, grepl("PSY|PHY", rownames(reduce_tf_idf)))
  
  # on réalise une 10-fold cross validation de l'algorithme k-mean
  results <- cross_validation_k_mean(reduce_tf_idf.class, 10)
  
  # on renvoie la moyenne des valeur d'AUC sur ces classes
  return(mean(as.double(results)))
}
```

```{r}
dim_min = 2
dim_max = 50
auc_by_dimentions <- mclapply(dim_min:dim_max, get_auc_by_dimention_kmean, tf_idf, mc.cores = detectCores())
plot(dim_min:dim_max, auc_by_dimentions, ylab = "AUC", xlab = "nombre de dimensions")
```
On peut voir que l'on approche presque 1 d'AUC sur plus de 35 dimentions, ca sera notre nombre optimal de dimensions

On peut maintenant se demander si on peut faire la réduction de dimention sur uniquement les cours PHY et PSY
- on réduis la matrice de tf-idf à seulement les PSY et les PHY
```{r}
tf_idf.class <- tf_idf[,grepl("PSY|PHY", rownames(t(tf_idf)))]
```

- on ressaye avec la dimension 50
```{r}
m.class.svd <- irlba(t(tf_idf.class), dim_redu)
reduce_tf_idf.class <- m.class.svd$u[, 1:dim_redu] %*% diag(m.class.svd$d[1:dim_redu])
rownames(reduce_tf_idf.class) <- colnames(tf_idf.class)

prediction_reduction <- apply_k_mean(reduce_tf_idf.class, 5, 5)
prediction_reduction.roc <- roc(prediction_reduction$is.PSY, prediction_reduction$pred.PSY, levels=c(0,1), direction="<", plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE, print.auc=TRUE, show.thres=TRUE)
```
On peut voir que l'AUC est nettement pire. Cela peut être due au sur-apprentissage. Comme nous avons réduis les dimentions des ensembles de test drastiquement, le sur-apprentissage est forcement plus important.

- on essaye sur différentes valeures de dimention
```{r}
dim_min = 10
dim_max = 50
auc_by_dimentions <- mclapply(dim_min:dim_max, get_auc_by_dimention_kmean, tf_idf.class, mc.cores = detectCores())
plot(dim_min:dim_max, auc_by_dimentions, ylab = "AUC", xlab = "nombre de dimensions")
```
On peut voir que cela a tendance à rendre notre modèle inconsistant, et le fait légèrement sur-apprendre, pour une valeur moyenne de 0.8 d'AUC apres 30 dimentions. La valeur optimale de dimentions latente serait donc autour de 10.