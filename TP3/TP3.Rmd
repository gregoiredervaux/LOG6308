---
title: "R Notebook"
output: html_notebook
---
# TP3

## Pré-traitement

### recupération des données

importation des modules
```{r}
library(Matrix)
library(NLP)
library(tm)
library(irlba)
library(stats)
library(pROC)
```

On construit des objects SimpleCorpus du package Text Mining

Sur ces coprus, on supprime les nombres, la ponctuation, et on tranfsorme toutes les majuscules en minuscules
```{r}
get_data <- function(src) {
  corpus <- SimpleCorpus(DirSource(src))
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(tolower))
  return(corpus)
}

corpus_poly <- get_data('data/Poly')
corpus_udm <- get_data('data/UdM')
corpus_hec <- get_data('data/HEC')
corpus_uqam <- get_data('data/UQAM')

head(data.frame(text = sapply(corpus_poly, as.character)))
```
On va ensuite construire un grand corpus regroupant les corpus des 4 écoles

```{r}
corpus_tot <- get_data(c('data/Poly', 'data/UdM', 'data/HEC', 'data/UQAM'))
```

### matrices Termes-Documents

On utilise le package TM pour construire des matrices termes-documents

On supprime avant cela les "stop words", mots très fréquents et que l'on retrouve dans tout les corpus de textes francais. Ces mots ne comportent pas de semantique particulière (comme les prépositions, etc...)
On supprime aussi les deux groupe nominaux "titrecours" et "descriptioncours" qui seront en quantité égale dans tout les documents et qui par conséquents ne sont pas utiles dans ce travail
```{r}
to_matrix <- function(corpus) {
  corpus <- tm_map(corpus,removeWords, stopwords('french'))
  corpus <- tm_map(corpus, removeWords, c('titrecours', 'descriptioncours'))
  return(TermDocumentMatrix(corpus))
}

td_poly <- to_matrix(corpus_poly)
td_udm <- to_matrix(corpus_udm)
td_hec <- to_matrix(corpus_hec)
td_uqam <- to_matrix(corpus_uqam)

td_tot <- to_matrix(corpus_tot)

```

Une fois la matrice terme-document définie, on va définir la matrice Tf-Idf

```{r}
get_Tf_IDF <- function(td) {
  tf <- as.matrix(td)
  tf_copy <- tf
  tf_copy[tf_copy != 0] <- 1
  n_i <- rowSums(tf_copy)
  idf <- log(nDocs(td_tot)/n_i)
  tf_idf = tf * idf
  return(tf_idf)
}

# tf_idf <- get_Tf_IDF(td_tot)
tf_idf <- Matrix(get_Tf_IDF(td_tot), "dgCMatrix")
head(tf_idf[, 0:5])
```

On reduis le nombre de dimention de la matrice. On transposera la matrice auparavant pour obtenir une matrice document-terme
```{r}
dim_redu = 50

m.svd <- irlba(t(tf_idf), 50)
reduce_tf_idf <- m.svd$u[, 1:dim_redu] %*% diag(m.svd$d[1:dim_redu])
rownames(reduce_tf_idf) <- colnames(tf_idf)
head(reduce_tf_idf[, 1:5])
```

## Question 1 

**Quels sont les 10 cours les plus similaires à LOG2420 dans l'espace réduit à 50 dimensions? **

On va dans un premier temps récupérer les fonctions de similarité de la correction du TP1
```{r}
## Cosinus entre un vecteur v et chaque colonne dela matrice m
cosinus.vm <- function(v,m) { n <- sqrt(colSums(m^2)); (v %*% m)/(n * sqrt(sum(v^2))) }
## Cosinus des colonnes d'une matrice
cosinus.mm <- function(m) { n <- sqrt(colSums(m^2)); crossprod(m)/(n %o% n) }
# Trouve les indexes des premières 'n' valeurs maximales d'une matrice
max.nindex <- function(m, n=5) {
  i <- order(m, decreasing=TRUE)
  return(i[1:n])
}
min.nindex <- function(m, n=5) {
  i <- order(m)
  return(i[1:n])
}
```

On va ensuite selectionner l'indice du cours LOG2420

```{r}
i <- grep('LOG2420', as.character(rownames(reduce_tf_idf)))
```

On va enfin calculer la similarité des documents entre-eux, avec la méthode des cosinus (on transpose la matrice pour obtenir une matrice terme-document)

```{r}
reduce_tf_idf.cos <- cosinus.mm(t(reduce_tf_idf))
```

on selectionne les 10 cours les plus proches de LOG2420

```{r}
i.sim.cos <- max.nindex(reduce_tf_idf.cos[,i], 11)
data.frame(Cos=reduce_tf_idf.cos[i.sim.cos, i], Index=i.sim.cos)
```

## Question 2

**Effectuez une classification de cours par une approche supervisée**

Dans un premier temps on va selectionner les index des cours qui nous intéressent

```{r}
reduce_tf_idf.class <- subset(reduce_tf_idf, grepl("PSY|PHY", rownames(reduce_tf_idf)))

reduce_tf_idf.PSY <- subset(reduce_tf_idf, grepl("PSY", rownames(reduce_tf_idf)))

reduce_tf_idf.PHY <- subset(reduce_tf_idf, grepl("PHY", rownames(reduce_tf_idf)))
```

On va maintenant calculer l'erreur des deux classes respectives:

- On sépare les données en entrainement et en test (le code est tiré de la correction du TP 1)

```{r}
nfolds = 10

split <- rep(1:nfolds, length.out=nrow(reduce_tf_idf.PSY))[sample(nrow(reduce_tf_idf.PSY))]
reduce_tf_idf.PSY.test.i <- (split==nfolds)
reduce_tf_idf.PSY.train.i <- !reduce_tf_idf.PSY.test.i

split <- rep(1:nfolds, length.out=nrow(reduce_tf_idf.PHY))[sample(nrow(reduce_tf_idf.PHY))]
reduce_tf_idf.PHY.test.i <- (split==nfolds)
reduce_tf_idf.PHY.train.i <- !reduce_tf_idf.PHY.test.i
```

On va ensuite calculer le centroides de la classes

```{r}
reduce_tf_idf.PSY.train <- reduce_tf_idf.PSY[reduce_tf_idf.PSY.train.i,]
reduce_tf_idf.PHY.train <- reduce_tf_idf.PHY[reduce_tf_idf.PHY.train.i,]

centroide.PSY <- colMeans(reduce_tf_idf.PSY[reduce_tf_idf.PSY.train.i,])
centroide.PHY <- colMeans(reduce_tf_idf.PHY[reduce_tf_idf.PHY.train.i,])
```

On va maintenant calculer l'erreur générée par cet itération

Pour chaque vecteur correspondant à une description de cours, on va calculer sa similarité cosinus avec les deux centroide. On prédira la classe en sélectionnant le centroide le plus proche

```{r}
reduce_tf_idf.PSY.test <- reduce_tf_idf.PSY[reduce_tf_idf.PSY.test.i,]
reduce_tf_idf.PHY.test <- reduce_tf_idf.PHY[reduce_tf_idf.PHY.test.i,]

cours <- rownames(reduce_tf_idf.PSY.test)
sim.PSY <- cosinus.vm(centroide.PSY, t(reduce_tf_idf.PSY.test))[1,]
sim.PHY <- cosinus.vm(centroide.PHY, t(reduce_tf_idf.PSY.test))[1,]

prediction.PSY <- data.frame(cours = rownames(reduce_tf_idf.PSY.test),
                                     sim.PSY = as.double(cosinus.vm(centroide.PSY, t(reduce_tf_idf.PSY.test))),
                                     sim.PHY = as.double(cosinus.vm(centroide.PHY, t(reduce_tf_idf.PSY.test))))

prediction.PSY$pred.PSY <- apply(prediction.PSY, 1, function (v) {as.numeric(v['sim.PSY']) / (as.numeric(v['sim.PHY']) + as.numeric(v['sim.PSY']))})
prediction.PSY$pred.PHY <- apply(prediction.PSY, 1, function (v) {as.numeric(v['sim.PHY']) / (as.numeric(v['sim.PHY']) + as.numeric(v['sim.PSY']))})
prediction.PSY$is.PSY <- 1
prediction.PSY$is.PHY <- 0


prediction.PHY <- data.frame(cours = rownames(reduce_tf_idf.PHY.test),
                                     sim.PSY = as.double(cosinus.vm(centroide.PSY, t(reduce_tf_idf.PHY.test))),
                                     sim.PHY = as.double(cosinus.vm(centroide.PHY, t(reduce_tf_idf.PHY.test))))

prediction.PHY$pred.PHY <- apply(prediction.PHY, 1, function (v) {as.numeric(v['sim.PHY']) / (as.numeric(v['sim.PHY']) + as.numeric(v['sim.PSY']))})
prediction.PHY$pred.PSY <- apply(prediction.PHY, 1, function (v) {as.numeric(v['sim.PSY']) / (as.numeric(v['sim.PHY']) + as.numeric(v['sim.PSY']))})
prediction.PHY$is.PHY <- 1
prediction.PHY$is.PSY <- 0

head(prediction.PHY)
```

Maintenant on peut construire des vecteurs pour fournir à la fonction ROC

```{r}
prediction.class <- rbind(prediction.PHY, prediction.PSY)
responce.PSY <- roc(prediction.class$is.PSY, prediction.class$pred.PSY, plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE, levels=c(0,1), direction="<")
responce.PHY <- roc(prediction.class$is.PHY, prediction.class$pred.PHY, plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE, levels=c(0,1), direction="<")
```
On peut remarquer que les deux courbes sont symétrique par rapport à x=-y. C'est normal car les deux probabilitées sont liées: P(psy) = 1 - P(phy)

Nous allons maintenant répeter toutes ces étapes n fois, en fonction du nombre de fold sélectionné
```{r}

get_auc <- function (m.PSY, m.PHY, nfolds) {
  result <- sapply(1:nfolds , function(fold) {
  
    # construction des ensemble d'apprentissage et de test
    
    split <- rep(1:nfolds, length.out=nrow(m.PSY))[sample(nrow(m.PSY))]
    m.PSY.test.i <- (split==fold)
    m.PSY.train.i <- !m.PSY.test.i
    
    m.PSY.test <- m.PSY[m.PSY.test.i,]
    m.PSY.train <- m.PSY[m.PSY.train.i,]
    
    split <- rep(1:nfolds, length.out=nrow(m.PHY))[sample(nrow(m.PHY))]
    m.PHY.test.i <- (split==fold)
    m.PHY.train.i <- !m.PHY.test.i
    
    m.PHY.test <- m.PHY[m.PHY.test.i,]
    m.PHY.train <- m.PHY[m.PHY.train.i,]
    
    # on determine les centroides
    centroide.PSY <- colMeans(m.PSY[m.PSY.train.i,])
    centroide.PHY <- colMeans(m.PHY[m.PHY.train.i,])
    
    # on construit un nouveau data frame pour PSY
    prediction.PSY <- data.frame(cours = rownames(m.PSY.test),
                                         sim.PSY = as.double(cosinus.vm(centroide.PSY, t(m.PSY.test))),
                                         sim.PHY = as.double(cosinus.vm(centroide.PHY, t(m.PSY.test))))
    
    prediction.PSY$pred.PSY <- apply(prediction.PSY, 1, function (v) {as.numeric(v['sim.PSY']) / (as.numeric(v['sim.PHY']) + as.numeric(v['sim.PSY']))})
    prediction.PSY$pred.PHY <- apply(prediction.PSY, 1, function (v) {as.numeric(v['sim.PHY']) / (as.numeric(v['sim.PHY']) + as.numeric(v['sim.PSY']))})
    prediction.PSY$is.PSY <- 1
    prediction.PSY$is.PHY <- 0
    
    # on construit un nouveau data frame pour PHY
    prediction.PHY <- data.frame(cours = rownames(m.PHY.test),
                                         sim.PSY = as.double(cosinus.vm(centroide.PSY, t(m.PHY.test))),
                                         sim.PHY = as.double(cosinus.vm(centroide.PHY, t(m.PHY.test))))
    
    prediction.PHY$pred.PHY <- apply(prediction.PHY, 1, function (v) {as.numeric(v['sim.PHY']) / (as.numeric(v['sim.PHY']) + as.numeric(v['sim.PSY']))})
    prediction.PHY$pred.PSY <- apply(prediction.PHY, 1, function (v) {as.numeric(v['sim.PSY']) / (as.numeric(v['sim.PHY']) + as.numeric(v['sim.PSY']))})
    prediction.PHY$is.PHY <- 1
    prediction.PHY$is.PSY <- 0
    
    # on recupère la valeur de AUC
    prediction.class <- rbind(prediction.PHY, prediction.PSY)
    responce.PSY <- roc(prediction.class$is.PSY, prediction.class$pred.PSY, levels=c(0,1), direction="<")
  })
  return(result)
}

results <- as.data.frame(t(get_auc(reduce_tf_idf.PSY, reduce_tf_idf.PHY, 10)))
mean(as.double(results$auc))
```
## Question 3

**À partir de la validation croisée de la tâche de classification ci-dessus, déterminez le nombre de dimensions latentes optimal de SVD selon une approche dite wrapper**

A partir de la fonction supérieure, nous allons essayer plusieures dimentions de SVD.

```{r}

get_auc_by_dimention <- function(dim_redu, tf_idf) {
  
  m.svd <- irlba(t(tf_idf), dim_redu)
  reduce_tf_idf <- m.svd$u[, 1:dim_redu] %*% diag(m.svd$d[1:dim_redu])
  rownames(reduce_tf_idf) <- colnames(tf_idf)

  reduce_tf_idf.PSY <- subset(reduce_tf_idf, grepl("PSY", rownames(reduce_tf_idf)))
  reduce_tf_idf.PHY <- subset(reduce_tf_idf, grepl("PHY", rownames(reduce_tf_idf)))
  
  results <- as.data.frame(t(get_auc(reduce_tf_idf.PSY, reduce_tf_idf.PHY, 10)))
  return(mean(as.double(results$auc)))
}
dim_min = 2
dim_max = 50
auc_by_dimentions <- lapply(dim_min:dim_max, get_auc_by_dimention, tf_idf)

plot(dim_min:dim_max, auc_by_dimentions)
```

## Question 4

**Effectuez une agglomération par k-means (k=2) et vérifiez si les classes PHY et PSY sont bien séparées par cette méthode**


```{r}

```
